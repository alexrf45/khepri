apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: ollama
  namespace: flux-system
spec:
  interval: 30m
  chart:
    spec:
      chart: app-template
      version: 4.2.0
      interval: 30m
      sourceRef:
        kind: HelmRepository
        name: bjw-s
        namespace: flux-system
  install:
    remediation:
      retries: 3
  upgrade:
    cleanupOnFail: true
    remediation:
      strategy: rollback
      retries: 3
  targetNamespace: ollama
  values:
    replicaCount: 1
    image:
      repository: ollama/ollama
      pullPolicy: Always`
      tag: ""
    nameOverride: ""
    fullnameOverride: ""
    namespaceOverride: ""
    ollama:
      port: 11434
      gpu:
        enabled: false
        draEnabled: false
        draDriverClass: "gpu.nvidia.com"
        draExistingClaimTemplate: ""
        type: 'nvidia'
        number: 1
        nvidiaResource: "nvidia.com/gpu"
        mig:
          enabled: false
          devices: {}
              #        1g.10gb: 1
              #        3g.40gb: 1
      models:
        # -- List of models to pull at container startup
        # The more you add, the longer the container will take to start if models are not present
        # pull:
        #  - llama2
        #  - mistral
        pull: []

        # -- List of models to load in memory at container startup
        # run:
        #  - llama2
        #  - mistral
        run: []

        # -- List of models to create at container startup, there are two options
        # 1. Create a raw model
        # 2. Load a model from configMaps, configMaps must be created before and are loaded as volume in "/models" directory.
        # create:
        #  - name: llama3.1-ctx32768
        #    configMapRef: my-configmap
        #    configMapKeyRef: configmap-key
        #  - name: llama3.1-ctx32768
        #    template: |
        #      FROM llama3.1
        #      PARAMETER num_ctx 32768
        create: []

        # -- Automatically remove models present on the disk but not specified in the values file
        clean: false

      # -- Add insecure flag for pulling at container startup
      insecure: false

      # -- Override ollama-data volume mount path, default: "/root/.ollama"
      mountPath: ""

    serviceAccount:
      create: true

      automount: false
      annotations: {}
    podAnnotations: {}
    podLabels: {}
    podSecurityContext: {}
    priorityClassName: ""
    securityContext: {}
      # capabilities:
      #  drop:
      #   - ALL
      # readOnlyRootFilesystem: true
      # runAsNonRoot: true
      # runAsUser: 1000

    # -- Specify runtime class
    runtimeClassName: ""
    service:
      type: ClusterIP
      port: 11434
      nodePort: 31434
      loadBalancerIP:
      annotations: {}
      labels: {}

    deployment:
      labels: {}

    # Configure resource requests and limits
    # ref: http://kubernetes.io/docs/user-guide/compute-resources/
    resources:
      # -- Pod requests
      requests: {}
        # Memory request
        # memory: 4096Mi

        # CPU request
        # cpu: 2000m

      # -- Pod limit
      limits: {}
        # Memory limit
        # memory: 8192Mi

        # CPU limit
        # cpu: 4000m

    persistentVolume:
      # -- Enable persistence using PVC
      enabled: false
      accessModes:
        - ReadWriteOnce
      annotations: {}
      existingClaim: ""
      size: 30Gi

      storageClass: "local-path"
      volumeMode: ""
      subPath: ""
      volumeName: ""
    nodeSelector: {}


    # Test connection pods
    tests:
      enabled: true
      labels: {}
      annotations: {}
